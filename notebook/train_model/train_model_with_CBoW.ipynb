{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42cd2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 10004\n",
      "Example tokenized sentence: ['នាយិកា', 'មជ្ឈមណ្ឌល', 'សិទ្ធិ', 'មនុស្ស', 'កម្ពុជា', 'អ្នកស្រី', 'ចក់', 'សុភាព', 'បង្ហាញ', 'តុលាការ', 'ក្រុង', 'ភ្នំពេញ', 'នាម', 'ជា', 'សាក្សី', 'សាក្សី', 'ផ្សេង', 'សំណុំរឿង', 'មេដឹកនាំ', 'នយោបាយ', 'ជំទាស់', 'កឹម', 'សុខា', 'ទីក្រុង', 'ភ្នំពេញ', 'ថ្ងៃទី', '៥', 'ខែតុលា', 'ឆ្នាំ', '២០២២']\n",
      "Word2Vec (CBoW) training complete.\n",
      "Shape of sentence vectors: (10004, 1000)\n",
      "Train size: 8003, Test size: 2001\n",
      "\n",
      "=== Word2Vec (CBoW) + Logistic Regression ===\n",
      "Accuracy: 0.46776611694152925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.51      0.50       651\n",
      "     neutral       0.16      0.52      0.24       184\n",
      "    positive       0.72      0.43      0.54      1166\n",
      "\n",
      "    accuracy                           0.47      2001\n",
      "   macro avg       0.45      0.49      0.43      2001\n",
      "weighted avg       0.59      0.47      0.50      2001\n",
      "\n",
      "\n",
      "=== Word2Vec (CBoW) + SVM ===\n",
      "Accuracy: 0.45327336331834084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.46      0.50      0.48       651\n",
      "     neutral       0.15      0.53      0.24       184\n",
      "    positive       0.72      0.42      0.53      1166\n",
      "\n",
      "    accuracy                           0.45      2001\n",
      "   macro avg       0.45      0.48      0.42      2001\n",
      "weighted avg       0.59      0.45      0.49      2001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======= Imports =======\n",
    "import numpy as np\n",
    "from khmernltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# ======= 1. Prepare sentences =======\n",
    "# data['clean_text'] should already exist\n",
    "data = pd.read_csv(\"../../data/cleaned_data.csv\")\n",
    "# sentences = data['text'].apply(word_tokenize).tolist()\n",
    "# Remove tokens that are just spaces\n",
    "sentences = [\n",
    "    [word for word in word_tokenize(text) if word.strip() != \"\"]\n",
    "    for text in data['text']\n",
    "]\n",
    "\n",
    "\n",
    "labels = data['label'].values\n",
    "\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"Example tokenized sentence: {sentences[0]}\")\n",
    "\n",
    "# ======= 2. Train Word2Vec (CBoW) =======\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=1000,  # embedding dimension\n",
    "    window=4,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=0  # 0 = CBoW\n",
    ")\n",
    "\n",
    "print(\"Word2Vec (CBoW) training complete.\")\n",
    "\n",
    "# ======= 3. Convert sentence to vector =======\n",
    "def sentence_vector(tokens, model):\n",
    "    \"\"\"Average word vectors for a sentence\"\"\"\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X = np.array([sentence_vector(s, w2v_model) for s in sentences])\n",
    "\n",
    "print(f\"Shape of sentence vectors: {X.shape}\")\n",
    "\n",
    "# ======= 4. Train/test split =======\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\n",
    "\n",
    "# ======= 5. Logistic Regression =======\n",
    "lr = LogisticRegression(max_iter=500, class_weight='balanced')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Word2Vec (CBoW) + Logistic Regression ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# ======= 6. SVM =======\n",
    "svm = SVC(kernel='linear', class_weight='balanced')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Word2Vec (CBoW) + SVM ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecc885fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\I5\\WR_Project\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n",
      "Classes: ['negative' 'neutral' 'positive']\n",
      "Vocabulary size: 13823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\I5\\WR_Project\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m1,382,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,400</span> (5.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,382,400\u001b[0m (5.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,400</span> (5.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,382,400\u001b[0m (5.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 286ms/step - accuracy: 0.5685 - loss: 0.8925 - val_accuracy: 0.5805 - val_loss: 0.8800\n",
      "Epoch 2/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 272ms/step - accuracy: 0.5816 - loss: 0.8703 - val_accuracy: 0.5743 - val_loss: 0.8634\n",
      "Epoch 3/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 272ms/step - accuracy: 0.5816 - loss: 0.8551 - val_accuracy: 0.5843 - val_loss: 0.8550\n",
      "Epoch 4/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 280ms/step - accuracy: 0.5796 - loss: 0.8485 - val_accuracy: 0.5893 - val_loss: 0.8481\n",
      "Epoch 5/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 292ms/step - accuracy: 0.5937 - loss: 0.8379 - val_accuracy: 0.5943 - val_loss: 0.8379\n",
      "Epoch 6/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 296ms/step - accuracy: 0.5943 - loss: 0.8380 - val_accuracy: 0.5905 - val_loss: 0.8408\n",
      "Epoch 7/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 294ms/step - accuracy: 0.5950 - loss: 0.8311 - val_accuracy: 0.5905 - val_loss: 0.8317\n",
      "Epoch 8/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 295ms/step - accuracy: 0.6023 - loss: 0.8262 - val_accuracy: 0.5993 - val_loss: 0.8348\n",
      "Epoch 9/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 304ms/step - accuracy: 0.6057 - loss: 0.8197 - val_accuracy: 0.5705 - val_loss: 0.8397\n",
      "Epoch 10/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 290ms/step - accuracy: 0.6077 - loss: 0.8168 - val_accuracy: 0.5993 - val_loss: 0.8336\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n",
      "\n",
      "=== LSTM Model ===\n",
      "Accuracy: 0.6111944027986007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.25      0.35       651\n",
      "     neutral       0.50      0.08      0.14       184\n",
      "    positive       0.62      0.90      0.73      1166\n",
      "\n",
      "    accuracy                           0.61      2001\n",
      "   macro avg       0.56      0.41      0.41      2001\n",
      "weighted avg       0.59      0.61      0.55      2001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\I5\\WR_Project\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m1,382,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,400</span> (5.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,382,400\u001b[0m (5.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,400</span> (5.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,382,400\u001b[0m (5.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 222ms/step - accuracy: 0.5761 - loss: 0.8965 - val_accuracy: 0.5793 - val_loss: 0.8719\n",
      "Epoch 2/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 197ms/step - accuracy: 0.5804 - loss: 0.8721 - val_accuracy: 0.5818 - val_loss: 0.8552\n",
      "Epoch 3/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 192ms/step - accuracy: 0.5861 - loss: 0.8533 - val_accuracy: 0.5918 - val_loss: 0.8478\n",
      "Epoch 4/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 190ms/step - accuracy: 0.5879 - loss: 0.8462 - val_accuracy: 0.5893 - val_loss: 0.8470\n",
      "Epoch 5/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 188ms/step - accuracy: 0.5937 - loss: 0.8416 - val_accuracy: 0.5680 - val_loss: 0.8433\n",
      "Epoch 6/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 197ms/step - accuracy: 0.5972 - loss: 0.8305 - val_accuracy: 0.5968 - val_loss: 0.8369\n",
      "Epoch 7/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 195ms/step - accuracy: 0.5975 - loss: 0.8282 - val_accuracy: 0.6005 - val_loss: 0.8389\n",
      "Epoch 8/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 192ms/step - accuracy: 0.6068 - loss: 0.8183 - val_accuracy: 0.5980 - val_loss: 0.8294\n",
      "Epoch 9/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 203ms/step - accuracy: 0.6084 - loss: 0.8136 - val_accuracy: 0.6042 - val_loss: 0.8303\n",
      "Epoch 10/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 223ms/step - accuracy: 0.6091 - loss: 0.8128 - val_accuracy: 0.6105 - val_loss: 0.8199\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
      "\n",
      "=== GRU Model ===\n",
      "Accuracy: 0.6216891554222889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.23      0.33       651\n",
      "     neutral       0.68      0.07      0.13       184\n",
      "    positive       0.62      0.93      0.74      1166\n",
      "\n",
      "    accuracy                           0.62      2001\n",
      "   macro avg       0.64      0.41      0.40      2001\n",
      "weighted avg       0.63      0.62      0.55      2001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======= Imports =======\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from khmernltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "\n",
    "# ======= 1. Load & prepare data =======\n",
    "data = pd.read_csv(\"../../data/cleaned_data.csv\")\n",
    "\n",
    "# Tokenize and remove empty tokens\n",
    "sentences = [\n",
    "    [word for word in word_tokenize(text) if word.strip() != \"\"]\n",
    "    for text in data['text']\n",
    "]\n",
    "\n",
    "texts = [\" \".join(s) for s in sentences]  # Join tokens for Keras tokenizer\n",
    "labels = data['label'].values\n",
    "\n",
    "# Encode labels to integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Classes:\", le.classes_)\n",
    "\n",
    "# ======= 2. Tokenizer + sequences =======\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100  # adjust based on average sentence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Vocabulary size:\", len(word_index))\n",
    "\n",
    "# ======= 3. Train Word2Vec (CBoW) =======\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=0  # CBoW\n",
    ")\n",
    "\n",
    "# ======= 4. Prepare embedding matrix =======\n",
    "embedding_dim = 100\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# ======= 5. Train/test split =======\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ======= 6. LSTM Model =======\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=num_words,\n",
    "                         output_dim=embedding_dim,\n",
    "                         weights=[embedding_matrix],\n",
    "                         input_length=MAX_SEQUENCE_LENGTH,\n",
    "                         trainable=False))  # freeze Word2Vec embeddings\n",
    "lstm_model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# ======= 7. Evaluate LSTM =======\n",
    "y_pred_lstm = np.argmax(lstm_model.predict(X_test), axis=1)\n",
    "print(\"\\n=== LSTM Model ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n",
    "print(classification_report(y_test, y_pred_lstm, target_names=le.classes_))\n",
    "\n",
    "# ======= 8. GRU Model =======\n",
    "gru_model = Sequential()\n",
    "gru_model.add(Embedding(input_dim=num_words,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                        trainable=False))\n",
    "gru_model.add(Bidirectional(GRU(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "gru_model.add(Dense(64, activation='relu'))\n",
    "gru_model.add(Dropout(0.2))\n",
    "gru_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "gru_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "gru_model.summary()\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# ======= 9. Evaluate GRU =======\n",
    "y_pred_gru = np.argmax(gru_model.predict(X_test), axis=1)\n",
    "print(\"\\n=== GRU Model ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gru))\n",
    "print(classification_report(y_test, y_pred_gru, target_names=le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wr-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
