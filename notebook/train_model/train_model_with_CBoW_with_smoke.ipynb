{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4e8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36be702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>នាយិកា មជ្ឈមណ្ឌល សិទ្ធិ មនុស្ស កម្ពុជា អ្នកស្រ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ការឃុំ កញ្ញា សេង ធារី កាន់តែ យូរ រដ្ឋាភិបាល ហ៊...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ប្រភព បង្ហើប បន្ទប់ ខ្ទង់ ចំណាយ ជាង ១០ម៉ឺន ដុល...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956 បាន បង្ហាញ ផូស្វ័រ បាន ផ្ទេរ ដើម បែក អារ ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ដរាបណា មិន បាន តាំងចិត្ត ខិតខំ ប្រឹង រៀន ប្រឹង...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  នាយិកា មជ្ឈមណ្ឌល សិទ្ធិ មនុស្ស កម្ពុជា អ្នកស្រ...   neutral\n",
       "1  ការឃុំ កញ្ញា សេង ធារី កាន់តែ យូរ រដ្ឋាភិបាល ហ៊...  positive\n",
       "2  ប្រភព បង្ហើប បន្ទប់ ខ្ទង់ ចំណាយ ជាង ១០ម៉ឺន ដុល...   neutral\n",
       "3  1956 បាន បង្ហាញ ផូស្វ័រ បាន ផ្ទេរ ដើម បែក អារ ...   neutral\n",
       "4  ដរាបណា មិន បាន តាំងចិត្ត ខិតខំ ប្រឹង រៀន ប្រឹង...  negative"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../../data/cleaned_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dbf54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and labels\n",
    "X = data['text']  # cleaned text\n",
    "y = data['label']      # labels\n",
    "\n",
    "# Split dataset: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y  # stratify keeps class distribution\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6d290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = [sentence.split() for sentence in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7510329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Word2Vec(\n",
    "    sentences=X_tokens,\n",
    "    vector_size=100,     # embedding dimension\n",
    "    window=5,            # context window size\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=0                 # sg=0 = CBoW\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "455cb30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, model, vector_size):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "    return np.mean(vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81bcd759",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = np.array([\n",
    "    sentence_to_vector(sentence, cbow_model, 100)\n",
    "    for sentence in X_train\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "074be7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X_vec, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "508cbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res,\n",
    "    y_res,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_res\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78f7b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=500, class_weight='balanced'),\n",
    "    \"LinearSVC\": LinearSVC(class_weight='balanced', max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20df1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LogisticRegression ===\n",
      "Accuracy: 0.5082201572551823\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.52      0.52       932\n",
      "     neutral       0.51      0.54      0.53       933\n",
      "    positive       0.48      0.46      0.47       933\n",
      "\n",
      "    accuracy                           0.51      2798\n",
      "   macro avg       0.51      0.51      0.51      2798\n",
      "weighted avg       0.51      0.51      0.51      2798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[487 226 219]\n",
      " [187 502 244]\n",
      " [252 248 433]]\n",
      "--------------------------------------------------\n",
      "=== LinearSVC ===\n",
      "Accuracy: 0.547891350964975\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.55      0.56       932\n",
      "     neutral       0.56      0.61      0.58       933\n",
      "    positive       0.52      0.48      0.50       933\n",
      "\n",
      "    accuracy                           0.55      2798\n",
      "   macro avg       0.55      0.55      0.55      2798\n",
      "weighted avg       0.55      0.55      0.55      2798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[514 220 198]\n",
      " [144 568 221]\n",
      " [250 232 451]]\n",
      "--------------------------------------------------\n",
      "=== RandomForest ===\n",
      "Accuracy: 0.8656182987848463\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.91      0.83       932\n",
      "     neutral       0.96      1.00      0.98       933\n",
      "    positive       0.89      0.69      0.77       933\n",
      "\n",
      "    accuracy                           0.87      2798\n",
      "   macro avg       0.87      0.87      0.86      2798\n",
      "weighted avg       0.87      0.87      0.86      2798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[847   2  83]\n",
      " [  0 933   0]\n",
      " [257  34 642]]\n",
      "--------------------------------------------------\n",
      "=== DecisionTree ===\n",
      "Accuracy: 0.8112937812723374\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.88      0.79       932\n",
      "     neutral       0.89      1.00      0.94       933\n",
      "    positive       0.85      0.56      0.67       933\n",
      "\n",
      "    accuracy                           0.81      2798\n",
      "   macro avg       0.82      0.81      0.80      2798\n",
      "weighted avg       0.82      0.81      0.80      2798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[817  23  92]\n",
      " [  0 933   0]\n",
      " [316  97 520]]\n",
      "--------------------------------------------------\n",
      "=== GradientBoosting ===\n",
      "Accuracy: 0.6147248034310222\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.65      0.61       932\n",
      "     neutral       0.67      0.75      0.71       933\n",
      "    positive       0.58      0.44      0.50       933\n",
      "\n",
      "    accuracy                           0.61      2798\n",
      "   macro avg       0.61      0.61      0.61      2798\n",
      "weighted avg       0.61      0.61      0.61      2798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[605 145 182]\n",
      " [113 704 116]\n",
      " [319 203 411]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77064ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model  accuracy  macro_f1\n",
      "2        RandomForest  0.865618  0.862508\n",
      "3        DecisionTree  0.811294  0.801333\n",
      "4    GradientBoosting  0.614725  0.608151\n",
      "1           LinearSVC  0.547891  0.546881\n",
      "0  LogisticRegression  0.508220  0.507877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": macro_f1\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.sort_values(\"macro_f1\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6522a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\I5\\WR_Project\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m117,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,635</span> (459.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m117,635\u001b[0m (459.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,635</span> (459.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m117,635\u001b[0m (459.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 59ms/step - accuracy: 0.3347 - loss: 1.0988 - val_accuracy: 0.3384 - val_loss: 1.0987\n",
      "Epoch 2/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - accuracy: 0.3309 - loss: 1.0990 - val_accuracy: 0.3384 - val_loss: 1.0982\n",
      "Epoch 3/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.3371 - loss: 1.0984 - val_accuracy: 0.3607 - val_loss: 1.0978\n",
      "Epoch 4/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.3584 - loss: 1.1007 - val_accuracy: 0.3330 - val_loss: 1.1308\n",
      "Epoch 5/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - accuracy: 0.3551 - loss: 1.1004 - val_accuracy: 0.3902 - val_loss: 1.0809\n",
      "Epoch 6/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.3952 - loss: 1.0804 - val_accuracy: 0.4589 - val_loss: 1.0359\n",
      "Epoch 7/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 0.4429 - loss: 1.0457 - val_accuracy: 0.4812 - val_loss: 1.0104\n",
      "Epoch 8/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.4602 - loss: 1.0342 - val_accuracy: 0.4812 - val_loss: 1.0095\n",
      "Epoch 9/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.4812 - loss: 1.0195 - val_accuracy: 0.4938 - val_loss: 0.9892\n",
      "Epoch 10/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.4874 - loss: 1.0137 - val_accuracy: 0.4991 - val_loss: 0.9930\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.62      0.55       932\n",
      "           1       0.55      0.65      0.60       933\n",
      "           2       0.38      0.21      0.27       933\n",
      "\n",
      "    accuracy                           0.50      2798\n",
      "   macro avg       0.48      0.50      0.47      2798\n",
      "weighted avg       0.48      0.50      0.47      2798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[579 189 164]\n",
      " [169 611 153]\n",
      " [417 318 198]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# -------------------------\n",
    "# 1. Tokenize sentences\n",
    "# -------------------------\n",
    "X_tokens = [sentence.split() for sentence in X_train]  # Khmer sentences tokenized\n",
    "\n",
    "# -------------------------\n",
    "# 2. Train CBoW Word2Vec\n",
    "# -------------------------\n",
    "vector_size = 100\n",
    "cbow_model = Word2Vec(sentences=X_tokens, vector_size=vector_size, window=5, min_count=2, workers=4, sg=0)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Convert sentences to sequence of embeddings\n",
    "# -------------------------\n",
    "def sentence_to_seq(sentence, model):\n",
    "    return [model.wv[word] for word in sentence.split() if word in model.wv]\n",
    "\n",
    "X_seq = [sentence_to_seq(sent, cbow_model) for sent in X_train]\n",
    "\n",
    "# Find max sentence length\n",
    "max_len = max(len(seq) for seq in X_seq)\n",
    "\n",
    "# Pad sequences with zeros\n",
    "X_padded = pad_sequences(\n",
    "    X_seq, maxlen=max_len, dtype='float32', padding='post', value=0.0\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4. Encode labels\n",
    "# -------------------------\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_train)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Oversample (optional)\n",
    "# -------------------------\n",
    "# Flatten for oversampling\n",
    "X_flat = X_padded.reshape(len(X_padded), -1)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X_flat, y_encoded)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_res = X_res.reshape(len(X_res), max_len, vector_size)\n",
    "y_res_cat = to_categorical(y_res)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Train-test split\n",
    "# -------------------------\n",
    "X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(\n",
    "    X_res, y_res_cat, test_size=0.2, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 7. Build LSTM / GRU model\n",
    "# -------------------------\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_len, vector_size), return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(y_train_dl.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# 8. Train model\n",
    "# -------------------------\n",
    "history = model.fit(\n",
    "    X_train_dl, y_train_dl,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 9. Evaluate\n",
    "# -------------------------\n",
    "y_pred_prob = model.predict(X_test_dl)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_true = np.argmax(y_test_dl, axis=1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa122b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wr-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
